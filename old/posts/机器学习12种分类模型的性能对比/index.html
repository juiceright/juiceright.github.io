<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>机器学习12种分类模型的性能对比 | Juiceright</title>
<meta name="keywords" content="old">
<meta name="description" content="接上回，我们已经知道了机器学习分类的本质，那么接下来就是要介绍机器学习中的分类模型的各种性能了。
简介
我们将采用12种分类模型，分别是：GaussianNB、MultinomialNB、KNNeighbors、SVC、DecisionTree、RandomForest、GradientBoosting、LGBM、XGB、CatBoost、AdaBoost、MLP，用这些模型对数据集进行训练，然后对测试集进行预测，最后将预测结果与真实结果进行对比，得到各个模型的性能。
数据集
我们采用以下代码生成随机分类数据：
x,y = make_classification(n_samples=100000,n_features=3,n_classes=4,n_informative=3,n_redundant=0,random_state=50,n_clusters_per_class=1)
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1)
这里能够生成100000个样本，每个样本有3个特征，4个类别，3个特征是有效特征，0个冗余特征，随机种子为50，每个类别有1个簇。然后我们将数据集分为训练集和测试集，测试集占10%.
由于每个样本都是一个3维向量，所以我们将数据取前2维数据将数据集可视化，如下图所示：

评估函数
我们采用准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1值(F1-score)、R2值(R2-score)，这5个指标来评估模型的性能。
准确率(Accuracy)
准确率是指分类正确的样本数占总样本数的比例，即：
$Accuracy = \frac{TP&#43;TN}{TP&#43;TN&#43;FP&#43;FN}$
其中，TP是真正例，TN是真负例，FP是假正例，FN是假负例。
精确率(Precision)
精确率是指分类正确的正例数占分类为正例的样本数的比例，即：

$$
Precision = \frac{TP}{TP&#43;FP}
$$

其中，TP是真正例，FP是假正例。
召回率(Recall)
召回率是指分类正确的正例数占真正例的比例，即：
$Recall = \frac{TP}{TP&#43;FN}$
其中，TP是真正例，FN是假负例。
F1值(F1-score)
F1值是精确率和召回率的调和平均数，即：

$$
F1 = \frac{2*Precision*Recall}{Precision&#43;Recall}
$$

F1值越接近1，表明模型的性能越好。
R2值(R2-score)
R2值是指预测值与真实值的相关系数，即：

$$
R2 = 1-\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}
$$

其中，$y_i$是真实值，$\hat{y_i}$是预测值，$\bar{y}$是真实值的均值。
R2值越接近1，表明模型的性能越好。
各模型表现
1. GaussianNB

其中GaussianNB的参数priors为None，var_smoothing为1e-09，表明我们没有对先验概率进行设置，而且我们对方差进行了平滑处理。
2. MultinomialNB

其中MultinomialNB的参数alpha为1.0，fit_prior为True，class_prior为None，表明我们对先验概率进行了设置，而且我们对先验概率进行了平滑处理。
3. KNNeighbors

其中KNNeighbors的参数n_neighbors为30，表明我们对最近邻的个数为30。
4. SVC

5. DecisionTree

6. RandomForest
">
<meta name="author" content="">
<link rel="canonical" href="https://juiceright.xyz/old/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css" integrity="sha256-NDzEgLn/yPBMy&#43;XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.gravatar.com/avatar/09752f12a50276f5fd5065e3afe3462a">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.gravatar.com/avatar/09752f12a50276f5fd5065e3afe3462a?s=16">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.gravatar.com/avatar/09752f12a50276f5fd5065e3afe3462a?s=32">
<link rel="apple-touch-icon" href="https://juiceright.xyz/apple-touch-icon.png">
<link rel="mask-icon" href="https://juiceright.xyz/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://juiceright.xyz/old/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><script type="text/javascript">
    (function(c,l,a,r,i,t,y){
        c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
        t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
        y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
    })(window, document, "clarity", "script", "v1t3dl05qv");
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-VW0CXHVZ33"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VW0CXHVZ33');
</script><meta property="og:url" content="https://juiceright.xyz/old/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/">
  <meta property="og:site_name" content="Juiceright">
  <meta property="og:title" content="机器学习12种分类模型的性能对比">
  <meta property="og:description" content="接上回，我们已经知道了机器学习分类的本质，那么接下来就是要介绍机器学习中的分类模型的各种性能了。
简介 我们将采用12种分类模型，分别是：GaussianNB、MultinomialNB、KNNeighbors、SVC、DecisionTree、RandomForest、GradientBoosting、LGBM、XGB、CatBoost、AdaBoost、MLP，用这些模型对数据集进行训练，然后对测试集进行预测，最后将预测结果与真实结果进行对比，得到各个模型的性能。
数据集 我们采用以下代码生成随机分类数据：
x,y = make_classification(n_samples=100000,n_features=3,n_classes=4,n_informative=3,n_redundant=0,random_state=50,n_clusters_per_class=1) x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1) 这里能够生成100000个样本，每个样本有3个特征，4个类别，3个特征是有效特征，0个冗余特征，随机种子为50，每个类别有1个簇。然后我们将数据集分为训练集和测试集，测试集占10%.
由于每个样本都是一个3维向量，所以我们将数据取前2维数据将数据集可视化，如下图所示：
评估函数 我们采用准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1值(F1-score)、R2值(R2-score)，这5个指标来评估模型的性能。
准确率(Accuracy) 准确率是指分类正确的样本数占总样本数的比例，即：
$Accuracy = \frac{TP&#43;TN}{TP&#43;TN&#43;FP&#43;FN}$
其中，TP是真正例，TN是真负例，FP是假正例，FN是假负例。
精确率(Precision) 精确率是指分类正确的正例数占分类为正例的样本数的比例，即：
$$ Precision = \frac{TP}{TP&#43;FP} $$ 其中，TP是真正例，FP是假正例。
召回率(Recall) 召回率是指分类正确的正例数占真正例的比例，即：
$Recall = \frac{TP}{TP&#43;FN}$
其中，TP是真正例，FN是假负例。
F1值(F1-score) F1值是精确率和召回率的调和平均数，即：
$$ F1 = \frac{2*Precision*Recall}{Precision&#43;Recall} $$ F1值越接近1，表明模型的性能越好。
R2值(R2-score) R2值是指预测值与真实值的相关系数，即：
$$ R2 = 1-\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2} $$ 其中，$y_i$是真实值，$\hat{y_i}$是预测值，$\bar{y}$是真实值的均值。
R2值越接近1，表明模型的性能越好。
各模型表现 1. GaussianNB 其中GaussianNB的参数priors为None，var_smoothing为1e-09，表明我们没有对先验概率进行设置，而且我们对方差进行了平滑处理。
2. MultinomialNB 其中MultinomialNB的参数alpha为1.0，fit_prior为True，class_prior为None，表明我们对先验概率进行了设置，而且我们对先验概率进行了平滑处理。
3. KNNeighbors 其中KNNeighbors的参数n_neighbors为30，表明我们对最近邻的个数为30。
4. SVC 5. DecisionTree 6. RandomForest ">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="old">
    <meta property="article:published_time" content="2023-06-19T22:42:48+00:00">
    <meta property="article:modified_time" content="2025-12-09T09:10:43+00:00">
    <meta property="article:tag" content="Old">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习12种分类模型的性能对比">
<meta name="twitter:description" content="接上回，我们已经知道了机器学习分类的本质，那么接下来就是要介绍机器学习中的分类模型的各种性能了。
简介
我们将采用12种分类模型，分别是：GaussianNB、MultinomialNB、KNNeighbors、SVC、DecisionTree、RandomForest、GradientBoosting、LGBM、XGB、CatBoost、AdaBoost、MLP，用这些模型对数据集进行训练，然后对测试集进行预测，最后将预测结果与真实结果进行对比，得到各个模型的性能。
数据集
我们采用以下代码生成随机分类数据：
x,y = make_classification(n_samples=100000,n_features=3,n_classes=4,n_informative=3,n_redundant=0,random_state=50,n_clusters_per_class=1)
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1)
这里能够生成100000个样本，每个样本有3个特征，4个类别，3个特征是有效特征，0个冗余特征，随机种子为50，每个类别有1个簇。然后我们将数据集分为训练集和测试集，测试集占10%.
由于每个样本都是一个3维向量，所以我们将数据取前2维数据将数据集可视化，如下图所示：

评估函数
我们采用准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1值(F1-score)、R2值(R2-score)，这5个指标来评估模型的性能。
准确率(Accuracy)
准确率是指分类正确的样本数占总样本数的比例，即：
$Accuracy = \frac{TP&#43;TN}{TP&#43;TN&#43;FP&#43;FN}$
其中，TP是真正例，TN是真负例，FP是假正例，FN是假负例。
精确率(Precision)
精确率是指分类正确的正例数占分类为正例的样本数的比例，即：

$$
Precision = \frac{TP}{TP&#43;FP}
$$

其中，TP是真正例，FP是假正例。
召回率(Recall)
召回率是指分类正确的正例数占真正例的比例，即：
$Recall = \frac{TP}{TP&#43;FN}$
其中，TP是真正例，FN是假负例。
F1值(F1-score)
F1值是精确率和召回率的调和平均数，即：

$$
F1 = \frac{2*Precision*Recall}{Precision&#43;Recall}
$$

F1值越接近1，表明模型的性能越好。
R2值(R2-score)
R2值是指预测值与真实值的相关系数，即：

$$
R2 = 1-\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}
$$

其中，$y_i$是真实值，$\hat{y_i}$是预测值，$\bar{y}$是真实值的均值。
R2值越接近1，表明模型的性能越好。
各模型表现
1. GaussianNB

其中GaussianNB的参数priors为None，var_smoothing为1e-09，表明我们没有对先验概率进行设置，而且我们对方差进行了平滑处理。
2. MultinomialNB

其中MultinomialNB的参数alpha为1.0，fit_prior为True，class_prior为None，表明我们对先验概率进行了设置，而且我们对先验概率进行了平滑处理。
3. KNNeighbors

其中KNNeighbors的参数n_neighbors为30，表明我们对最近邻的个数为30。
4. SVC

5. DecisionTree

6. RandomForest
">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Olds",
      "item": "https://juiceright.xyz/old/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "机器学习12种分类模型的性能对比",
      "item": "https://juiceright.xyz/old/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "机器学习12种分类模型的性能对比",
  "name": "机器学习12种分类模型的性能对比",
  "description": "接上回，我们已经知道了机器学习分类的本质，那么接下来就是要介绍机器学习中的分类模型的各种性能了。\n简介 我们将采用12种分类模型，分别是：GaussianNB、MultinomialNB、KNNeighbors、SVC、DecisionTree、RandomForest、GradientBoosting、LGBM、XGB、CatBoost、AdaBoost、MLP，用这些模型对数据集进行训练，然后对测试集进行预测，最后将预测结果与真实结果进行对比，得到各个模型的性能。\n数据集 我们采用以下代码生成随机分类数据：\nx,y = make_classification(n_samples=100000,n_features=3,n_classes=4,n_informative=3,n_redundant=0,random_state=50,n_clusters_per_class=1) x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1) 这里能够生成100000个样本，每个样本有3个特征，4个类别，3个特征是有效特征，0个冗余特征，随机种子为50，每个类别有1个簇。然后我们将数据集分为训练集和测试集，测试集占10%.\n由于每个样本都是一个3维向量，所以我们将数据取前2维数据将数据集可视化，如下图所示：\n评估函数 我们采用准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1值(F1-score)、R2值(R2-score)，这5个指标来评估模型的性能。\n准确率(Accuracy) 准确率是指分类正确的样本数占总样本数的比例，即：\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$\n其中，TP是真正例，TN是真负例，FP是假正例，FN是假负例。\n精确率(Precision) 精确率是指分类正确的正例数占分类为正例的样本数的比例，即：\n$$ Precision = \\frac{TP}{TP+FP} $$ 其中，TP是真正例，FP是假正例。\n召回率(Recall) 召回率是指分类正确的正例数占真正例的比例，即：\n$Recall = \\frac{TP}{TP+FN}$\n其中，TP是真正例，FN是假负例。\nF1值(F1-score) F1值是精确率和召回率的调和平均数，即：\n$$ F1 = \\frac{2*Precision*Recall}{Precision+Recall} $$ F1值越接近1，表明模型的性能越好。\nR2值(R2-score) R2值是指预测值与真实值的相关系数，即：\n$$ R2 = 1-\\frac{\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i-\\bar{y})^2} $$ 其中，$y_i$是真实值，$\\hat{y_i}$是预测值，$\\bar{y}$是真实值的均值。\nR2值越接近1，表明模型的性能越好。\n各模型表现 1. GaussianNB 其中GaussianNB的参数priors为None，var_smoothing为1e-09，表明我们没有对先验概率进行设置，而且我们对方差进行了平滑处理。\n2. MultinomialNB 其中MultinomialNB的参数alpha为1.0，fit_prior为True，class_prior为None，表明我们对先验概率进行了设置，而且我们对先验概率进行了平滑处理。\n3. KNNeighbors 其中KNNeighbors的参数n_neighbors为30，表明我们对最近邻的个数为30。\n4. SVC 5. DecisionTree 6. RandomForest ",
  "keywords": [
    "old"
  ],
  "articleBody": "接上回，我们已经知道了机器学习分类的本质，那么接下来就是要介绍机器学习中的分类模型的各种性能了。\n简介 我们将采用12种分类模型，分别是：GaussianNB、MultinomialNB、KNNeighbors、SVC、DecisionTree、RandomForest、GradientBoosting、LGBM、XGB、CatBoost、AdaBoost、MLP，用这些模型对数据集进行训练，然后对测试集进行预测，最后将预测结果与真实结果进行对比，得到各个模型的性能。\n数据集 我们采用以下代码生成随机分类数据：\nx,y = make_classification(n_samples=100000,n_features=3,n_classes=4,n_informative=3,n_redundant=0,random_state=50,n_clusters_per_class=1) x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1) 这里能够生成100000个样本，每个样本有3个特征，4个类别，3个特征是有效特征，0个冗余特征，随机种子为50，每个类别有1个簇。然后我们将数据集分为训练集和测试集，测试集占10%.\n由于每个样本都是一个3维向量，所以我们将数据取前2维数据将数据集可视化，如下图所示：\n评估函数 我们采用准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1值(F1-score)、R2值(R2-score)，这5个指标来评估模型的性能。\n准确率(Accuracy) 准确率是指分类正确的样本数占总样本数的比例，即：\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$\n其中，TP是真正例，TN是真负例，FP是假正例，FN是假负例。\n精确率(Precision) 精确率是指分类正确的正例数占分类为正例的样本数的比例，即：\n$$ Precision = \\frac{TP}{TP+FP} $$ 其中，TP是真正例，FP是假正例。\n召回率(Recall) 召回率是指分类正确的正例数占真正例的比例，即：\n$Recall = \\frac{TP}{TP+FN}$\n其中，TP是真正例，FN是假负例。\nF1值(F1-score) F1值是精确率和召回率的调和平均数，即：\n$$ F1 = \\frac{2*Precision*Recall}{Precision+Recall} $$ F1值越接近1，表明模型的性能越好。\nR2值(R2-score) R2值是指预测值与真实值的相关系数，即：\n$$ R2 = 1-\\frac{\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i-\\bar{y})^2} $$ 其中，$y_i$是真实值，$\\hat{y_i}$是预测值，$\\bar{y}$是真实值的均值。\nR2值越接近1，表明模型的性能越好。\n各模型表现 1. GaussianNB 其中GaussianNB的参数priors为None，var_smoothing为1e-09，表明我们没有对先验概率进行设置，而且我们对方差进行了平滑处理。\n2. MultinomialNB 其中MultinomialNB的参数alpha为1.0，fit_prior为True，class_prior为None，表明我们对先验概率进行了设置，而且我们对先验概率进行了平滑处理。\n3. KNNeighbors 其中KNNeighbors的参数n_neighbors为30，表明我们对最近邻的个数为30。\n4. SVC 5. DecisionTree 6. RandomForest 7. GradientBoosting 8. LGBM 9. XGB 10. CatBoost 11. AdaBoost 12. MLP 总结 MLP(神经网络模型)在准确率、精确率、召回率、F1值、R2值上的总体表现最好，这归于神经网络模型的强大的拟合能力。\n其次是KNNeighbors(K近邻模型)，K近邻模型的各项指标都很高，而且更值得一提的是：K近邻模型的训练速度非常快，也是所有模型中原理最简单的。\n总体来说：判别模型的性能要优于生成模型，这是因为判别模型的假设空间更小，所以更容易拟合数据。\n有一点猜想：生成模型是基于贝叶斯公式的，而贝叶斯公式是基于条件概率的，而条件概率是基于联合概率的。当样本数量很大时，样本噪声也会很大，这样计算联合概率就会很困难，计算概率的准确率也会很低，所以生成模型的性能就会很差。\n# %% #加载Intel的scikit-learn加速 from sklearnex import patch_sklearn patch_sklearn(global_patch=True) # %% #用sklearn随机生成一个有3个分类的数据集，然后用KNN算法进行分类 from sklearn.datasets import make_classification from sklearn.metrics import accuracy_score from sklearn.metrics import r2_score # R2评分，R2值越接近1，表示模型越好，越接近0，表示模型越差 from sklearn.metrics import precision_score from sklearn.metrics import recall_score from sklearn.metrics import f1_score x,y = make_classification(n_samples=100000,n_features=3,n_classes=4,n_informative=3,n_redundant=0,random_state=50,n_clusters_per_class=1) #取其中的两个维度进行绘图 import matplotlib.pyplot as plt plt.title('Make_classification Data') plt.scatter(x[:,0],x[:,1],marker='.',c=y) plt.show() from sklearn.model_selection import train_test_split x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1) # %% #将数据归一化，数据都是正数 from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() scaler.fit(x_train) x_train = scaler.transform(x_train) x_test = scaler.transform(x_test) # %% def accurate(title): plt.title(title) plt.scatter(x_test[:, 0], x_test[:, 1], marker='.', c=y_predict) plt.text(0.7, 0.7, 'Accuracy:%.5f' % accuracy_score(y_test, y_predict)) plt.text(0.7, 0.75, 'Precision:%.5f' % precision_score(y_test, y_predict, average='macro')) plt.text(0.7, 0.8, 'Recall:%.5f' % recall_score(y_test, y_predict, average='macro')) plt.text(0.7, 0.85, 'F1 score:%.5f' % f1_score(y_test, y_predict, average='macro')) plt.text(0.7, 0.9, 'R2 score:%.5f' % r2_score(y_test, y_predict)) plt.show() # %% # 用高斯贝叶斯算法进行分类 from sklearn.naive_bayes import GaussianNB, MultinomialNB gnb = GaussianNB() gnb.fit(x_train, y_train) y_predict = gnb.predict(x_test) accurate('GaussianNB') # %% #用多项式贝叶斯算法进行分类 mnb = MultinomialNB() mnb.fit(x_train,y_train) y_predict = mnb.predict(x_test) accurate('MultinomialNB') # %% #用K近邻算法进行分类 from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=30) knn.fit(x_train,y_train) y_predict = knn.predict(x_test) accurate('KNeighborsClassifier') # %% #用SVM算法进行分类 from sklearn.svm import SVC svm = SVC() svm.fit(x_train,y_train) y_predict = svm.predict(x_test) accurate('SVC') # %% #用决策树算法进行分类 from sklearn.tree import DecisionTreeClassifier dtc = DecisionTreeClassifier() dtc.fit(x_train,y_train) y_predict = dtc.predict(x_test) accurate('DecisionTreeClassifier') # %% #用随机森林算法进行分类 from sklearn.ensemble import RandomForestClassifier rfc = RandomForestClassifier() rfc.fit(x_train,y_train) y_predict = rfc.predict(x_test) accurate('RandomForestClassifier') # %% #用梯度提升算法进行分类 from sklearn.ensemble import GradientBoostingClassifier gbc = GradientBoostingClassifier() gbc.fit(x_train,y_train) y_predict = gbc.predict(x_test) accurate('GradientBoostingClassifier') # %% #用LightGBM算法进行分类 from lightgbm import LGBMClassifier lgbmc = LGBMClassifier() lgbmc.fit(x_train,y_train) y_predict = lgbmc.predict(x_test) accurate('LGBMClassifier') # %% #用XGBoost算法进行分类 from xgboost import XGBClassifier xgbc = XGBClassifier() xgbc.fit(x_train,y_train) y_predict = xgbc.predict(x_test) accurate('XGBClassifier') # %% #用CatBoost算法进行分类 from catboost import CatBoostClassifier cbc = CatBoostClassifier() cbc.fit(x_train,y_train) y_predict = cbc.predict(x_test) # %% accurate('CatBoostClassifier') # %% #用AdaBoost算法进行分类 from sklearn.ensemble import AdaBoostClassifier abc = AdaBoostClassifier() abc.fit(x_train,y_train) y_predict = abc.predict(x_test) accurate('AdaBoostClassifier') # %% #用神经网络算法进行分类 from sklearn.neural_network import MLPClassifier mlp = MLPClassifier() mlp.fit(x_train,y_train) y_predict = mlp.predict(x_test) accurate('MLPClassifier') ",
  "wordCount" : "386",
  "inLanguage": "en",
  "datePublished": "2023-06-19T22:42:48Z",
  "dateModified": "2025-12-09T09:10:43.029Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://juiceright.xyz/old/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Juiceright",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.gravatar.com/avatar/09752f12a50276f5fd5065e3afe3462a"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://juiceright.xyz/" accesskey="h" title="Juiceright (Alt + H)">Juiceright</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://juiceright.xyz/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://juiceright.xyz/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://juiceright.xyz/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://juiceright.xyz/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      机器学习12种分类模型的性能对比
    </h1>
    <div class="post-meta"><span title='2023-06-19 22:42:48 +0000 UTC'>June 19, 2023</span>&nbsp;·&nbsp;<span>2 min</span>

</div>
  </header> 
  <div class="post-content"><p>接上回，我们已经知道了<a href="/2023/05/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB%E7%9A%84%E6%9C%AC%E8%B4%A8/">机器学习分类的本质</a>，那么接下来就是要介绍机器学习中的分类模型的各种性能了。</p>
<h2 id="简介">简介<a hidden class="anchor" aria-hidden="true" href="#简介">#</a></h2>
<p>我们将采用12种分类模型，分别是：GaussianNB、MultinomialNB、KNNeighbors、SVC、DecisionTree、RandomForest、GradientBoosting、LGBM、XGB、CatBoost、AdaBoost、MLP，用这些模型对数据集进行训练，然后对测试集进行预测，最后将预测结果与真实结果进行对比，得到各个模型的性能。</p>
<h2 id="数据集">数据集<a hidden class="anchor" aria-hidden="true" href="#数据集">#</a></h2>
<p>我们采用以下代码生成随机分类数据：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x,y <span style="color:#f92672">=</span> make_classification(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">100000</span>,n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,n_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,n_informative<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,n_redundant<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>,n_clusters_per_class<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>x_train,x_test,y_train,y_test <span style="color:#f92672">=</span> train_test_split(x,y,test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
</span></span></code></pre></div><p>这里能够生成100000个样本，每个样本有3个特征，4个类别，3个特征是有效特征，0个冗余特征，随机种子为50，每个类别有1个簇。然后我们将数据集分为训练集和测试集，测试集占10%.<br>
由于每个样本都是一个3维向量，所以我们将数据取前2维数据将数据集可视化，如下图所示：<br>
<img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171042912.png"></p>
<h2 id="评估函数">评估函数<a hidden class="anchor" aria-hidden="true" href="#评估函数">#</a></h2>
<p>我们采用准确率(Accuracy)、精确率(Precision)、召回率(Recall)、F1值(F1-score)、R2值(R2-score)，这5个指标来评估模型的性能。</p>
<h4 id="准确率accuracy">准确率(Accuracy)<a hidden class="anchor" aria-hidden="true" href="#准确率accuracy">#</a></h4>
<p>准确率是指分类正确的样本数占总样本数的比例，即：</p>
<p>$Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$</p>
<p>其中，TP是真正例，TN是真负例，FP是假正例，FN是假负例。</p>
<h4 id="精确率precision">精确率(Precision)<a hidden class="anchor" aria-hidden="true" href="#精确率precision">#</a></h4>
<p>精确率是指分类正确的正例数占分类为正例的样本数的比例，即：</p>
<div>
$$
Precision = \frac{TP}{TP+FP}
$$
</div>
<p>其中，TP是真正例，FP是假正例。</p>
<h4 id="召回率recall">召回率(Recall)<a hidden class="anchor" aria-hidden="true" href="#召回率recall">#</a></h4>
<p>召回率是指分类正确的正例数占真正例的比例，即：</p>
<p>$Recall = \frac{TP}{TP+FN}$</p>
<p>其中，TP是真正例，FN是假负例。</p>
<h4 id="f1值f1-score">F1值(F1-score)<a hidden class="anchor" aria-hidden="true" href="#f1值f1-score">#</a></h4>
<p>F1值是精确率和召回率的调和平均数，即：</p>
<div>
$$
F1 = \frac{2*Precision*Recall}{Precision+Recall}
$$
</div>
<p>F1值越接近1，表明模型的性能越好。</p>
<h4 id="r2值r2-score">R2值(R2-score)<a hidden class="anchor" aria-hidden="true" href="#r2值r2-score">#</a></h4>
<p>R2值是指预测值与真实值的相关系数，即：</p>
<div>
$$
R2 = 1-\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{\sum_{i=1}^{n}(y_i-\bar{y})^2}
$$
</div>
<p>其中，$y_i$是真实值，$\hat{y_i}$是预测值，$\bar{y}$是真实值的均值。</p>
<p>R2值越接近1，表明模型的性能越好。</p>
<h2 id="各模型表现">各模型表现<a hidden class="anchor" aria-hidden="true" href="#各模型表现">#</a></h2>
<h3 id="1-gaussiannb">1. GaussianNB<a hidden class="anchor" aria-hidden="true" href="#1-gaussiannb">#</a></h3>
<p><img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171042918.png"><br>
其中GaussianNB的参数priors为None，var_smoothing为1e-09，表明我们没有对先验概率进行设置，而且我们对方差进行了平滑处理。</p>
<h3 id="2-multinomialnb">2. MultinomialNB<a hidden class="anchor" aria-hidden="true" href="#2-multinomialnb">#</a></h3>
<p><img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171042927.png"><br>
其中MultinomialNB的参数alpha为1.0，fit_prior为True，class_prior为None，表明我们对先验概率进行了设置，而且我们对先验概率进行了平滑处理。</p>
<h3 id="3-knneighbors">3. KNNeighbors<a hidden class="anchor" aria-hidden="true" href="#3-knneighbors">#</a></h3>
<p><img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171042934.png"><br>
其中KNNeighbors的参数n_neighbors为30，表明我们对最近邻的个数为30。</p>
<h3 id="4-svc">4. SVC<a hidden class="anchor" aria-hidden="true" href="#4-svc">#</a></h3>
<p><img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171042943.png"></p>
<h3 id="5-decisiontree">5. DecisionTree<a hidden class="anchor" aria-hidden="true" href="#5-decisiontree">#</a></h3>
<p><img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171042951.png"></p>
<h3 id="6-randomforest">6. RandomForest<a hidden class="anchor" aria-hidden="true" href="#6-randomforest">#</a></h3>
<p><img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171042964.png"></p>
<h3 id="7-gradientboosting">7. GradientBoosting<a hidden class="anchor" aria-hidden="true" href="#7-gradientboosting">#</a></h3>
<p><img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171042973.png"></p>
<h3 id="8-lgbm">8. LGBM<a hidden class="anchor" aria-hidden="true" href="#8-lgbm">#</a></h3>
<p><img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171042982.png"></p>
<h3 id="9-xgb">9. XGB<a hidden class="anchor" aria-hidden="true" href="#9-xgb">#</a></h3>
<p><img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171042990.png"></p>
<h3 id="10-catboost">10. CatBoost<a hidden class="anchor" aria-hidden="true" href="#10-catboost">#</a></h3>
<p><img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171042998.png"></p>
<h3 id="11-adaboost">11. AdaBoost<a hidden class="anchor" aria-hidden="true" href="#11-adaboost">#</a></h3>
<p><img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171043013.png"></p>
<h3 id="12-mlp">12. MLP<a hidden class="anchor" aria-hidden="true" href="#12-mlp">#</a></h3>
<p><img loading="lazy" src="/images/202512/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012%E7%A7%8D%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94-20251209171043022.png"></p>
<h2 id="总结">总结<a hidden class="anchor" aria-hidden="true" href="#总结">#</a></h2>
<p>MLP(神经网络模型)在准确率、精确率、召回率、F1值、R2值上的总体表现最好，这归于神经网络模型的强大的拟合能力。<br>
其次是KNNeighbors(K近邻模型)，K近邻模型的各项指标都很高，而且更值得一提的是：K近邻模型的训练速度非常快，也是所有模型中原理最简单的。</p>
<p>总体来说：判别模型的性能要优于生成模型，这是因为判别模型的假设空间更小，所以更容易拟合数据。</p>
<p>有一点猜想：生成模型是基于贝叶斯公式的，而贝叶斯公式是基于条件概率的，而条件概率是基于联合概率的。当样本数量很大时，样本噪声也会很大，这样计算联合概率就会很困难，计算概率的准确率也会很低，所以生成模型的性能就会很差。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#加载Intel的scikit-learn加速</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearnex <span style="color:#f92672">import</span> patch_sklearn 
</span></span><span style="display:flex;"><span>patch_sklearn(global_patch<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用sklearn随机生成一个有3个分类的数据集，然后用KNN算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> make_classification
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> accuracy_score
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> r2_score <span style="color:#75715e"># R2评分，R2值越接近1，表示模型越好，越接近0，表示模型越差</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> precision_score
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> recall_score 
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> f1_score 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x,y <span style="color:#f92672">=</span> make_classification(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">100000</span>,n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,n_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,n_informative<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,n_redundant<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>,n_clusters_per_class<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#取其中的两个维度进行绘图</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Make_classification Data&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(x[:,<span style="color:#ae81ff">0</span>],x[:,<span style="color:#ae81ff">1</span>],marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>,c<span style="color:#f92672">=</span>y)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span>x_train,x_test,y_train,y_test <span style="color:#f92672">=</span> train_test_split(x,y,test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#将数据归一化，数据都是正数</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> MinMaxScaler
</span></span><span style="display:flex;"><span>scaler <span style="color:#f92672">=</span> MinMaxScaler() 
</span></span><span style="display:flex;"><span>scaler<span style="color:#f92672">.</span>fit(x_train) 
</span></span><span style="display:flex;"><span>x_train <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>transform(x_train) 
</span></span><span style="display:flex;"><span>x_test <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>transform(x_test) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">accurate</span>(title):
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(title)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>scatter(x_test[:, <span style="color:#ae81ff">0</span>], x_test[:, <span style="color:#ae81ff">1</span>], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>, c<span style="color:#f92672">=</span>y_predict)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.7</span>, <span style="color:#ae81ff">0.7</span>, <span style="color:#e6db74">&#39;Accuracy:</span><span style="color:#e6db74">%.5f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> accuracy_score(y_test, y_predict)) 
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.7</span>, <span style="color:#ae81ff">0.75</span>, <span style="color:#e6db74">&#39;Precision:</span><span style="color:#e6db74">%.5f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> precision_score(y_test, y_predict, average<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;macro&#39;</span>)) 
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.7</span>, <span style="color:#ae81ff">0.8</span>, <span style="color:#e6db74">&#39;Recall:</span><span style="color:#e6db74">%.5f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> recall_score(y_test, y_predict, average<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;macro&#39;</span>)) 
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.7</span>, <span style="color:#ae81ff">0.85</span>, <span style="color:#e6db74">&#39;F1 score:</span><span style="color:#e6db74">%.5f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> f1_score(y_test, y_predict, average<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;macro&#39;</span>)) 
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.7</span>, <span style="color:#ae81ff">0.9</span>, <span style="color:#e6db74">&#39;R2 score:</span><span style="color:#e6db74">%.5f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> r2_score(y_test, y_predict))
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 用高斯贝叶斯算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.naive_bayes <span style="color:#f92672">import</span> GaussianNB, MultinomialNB
</span></span><span style="display:flex;"><span>gnb <span style="color:#f92672">=</span> GaussianNB()
</span></span><span style="display:flex;"><span>gnb<span style="color:#f92672">.</span>fit(x_train, y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> gnb<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;GaussianNB&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用多项式贝叶斯算法进行分类</span>
</span></span><span style="display:flex;"><span>mnb <span style="color:#f92672">=</span> MultinomialNB() 
</span></span><span style="display:flex;"><span>mnb<span style="color:#f92672">.</span>fit(x_train,y_train) 
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> mnb<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;MultinomialNB&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用K近邻算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.neighbors <span style="color:#f92672">import</span> KNeighborsClassifier
</span></span><span style="display:flex;"><span>knn <span style="color:#f92672">=</span> KNeighborsClassifier(n_neighbors<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>)
</span></span><span style="display:flex;"><span>knn<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> knn<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;KNeighborsClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用SVM算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.svm <span style="color:#f92672">import</span> SVC
</span></span><span style="display:flex;"><span>svm <span style="color:#f92672">=</span> SVC()
</span></span><span style="display:flex;"><span>svm<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> svm<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;SVC&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用决策树算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeClassifier
</span></span><span style="display:flex;"><span>dtc <span style="color:#f92672">=</span> DecisionTreeClassifier()
</span></span><span style="display:flex;"><span>dtc<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> dtc<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;DecisionTreeClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用随机森林算法进行分类 </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> RandomForestClassifier 
</span></span><span style="display:flex;"><span>rfc <span style="color:#f92672">=</span> RandomForestClassifier() 
</span></span><span style="display:flex;"><span>rfc<span style="color:#f92672">.</span>fit(x_train,y_train) 
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> rfc<span style="color:#f92672">.</span>predict(x_test) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;RandomForestClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用梯度提升算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> GradientBoostingClassifier 
</span></span><span style="display:flex;"><span>gbc <span style="color:#f92672">=</span> GradientBoostingClassifier() 
</span></span><span style="display:flex;"><span>gbc<span style="color:#f92672">.</span>fit(x_train,y_train) 
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> gbc<span style="color:#f92672">.</span>predict(x_test) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;GradientBoostingClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用LightGBM算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> lightgbm <span style="color:#f92672">import</span> LGBMClassifier
</span></span><span style="display:flex;"><span>lgbmc <span style="color:#f92672">=</span> LGBMClassifier()
</span></span><span style="display:flex;"><span>lgbmc<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> lgbmc<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;LGBMClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用XGBoost算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> xgboost <span style="color:#f92672">import</span> XGBClassifier 
</span></span><span style="display:flex;"><span>xgbc <span style="color:#f92672">=</span> XGBClassifier() 
</span></span><span style="display:flex;"><span>xgbc<span style="color:#f92672">.</span>fit(x_train,y_train) 
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> xgbc<span style="color:#f92672">.</span>predict(x_test) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;XGBClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用CatBoost算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> catboost <span style="color:#f92672">import</span> CatBoostClassifier
</span></span><span style="display:flex;"><span>cbc <span style="color:#f92672">=</span> CatBoostClassifier()
</span></span><span style="display:flex;"><span>cbc<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> cbc<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;CatBoostClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用AdaBoost算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> AdaBoostClassifier
</span></span><span style="display:flex;"><span>abc <span style="color:#f92672">=</span> AdaBoostClassifier()
</span></span><span style="display:flex;"><span>abc<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> abc<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;AdaBoostClassifier&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># %%</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#用神经网络算法进行分类</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.neural_network <span style="color:#f92672">import</span> MLPClassifier
</span></span><span style="display:flex;"><span>mlp <span style="color:#f92672">=</span> MLPClassifier()
</span></span><span style="display:flex;"><span>mlp<span style="color:#f92672">.</span>fit(x_train,y_train)
</span></span><span style="display:flex;"><span>y_predict <span style="color:#f92672">=</span> mlp<span style="color:#f92672">.</span>predict(x_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accurate(<span style="color:#e6db74">&#39;MLPClassifier&#39;</span>)
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://juiceright.xyz/tags/old/">Old</a></li>
    </ul>
  </footer><div id="gitalk-container"></div>
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script>
  const gitalk = new Gitalk({
    clientID: 'Ov23liuWJuk8S1oC4VRp',
    clientSecret: 'dd9a460fc89c99acdf580410cffacf5f1e9aa032',
    repo: 'juiceright.github.io',
    owner: 'juiceright',
    admin: ['juiceright'],
    id: decodeURI(location.pathname), 
    distractionFreeMode: true 
  });
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('gitalk-container').innerHTML = 'Gitalk comments not available by default when the website is previewed locally.';
      return;
    }
    gitalk.render('gitalk-container');
  })();
</script>

</article>
    </main>
    

<footer class="footer">
        <span>&copy; 2026 <a href="https://juiceright.xyz/">Juiceright</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
        
        <div id="busuanzi_stats" style="display:none;"> ·  <span>👥 <span id="busuanzi_value_site_uv"></span></span> <span>|</span> <span>🌐 <span id="busuanzi_value_site_pv"></span></span> <span>|</span> <span>📖 <span id="busuanzi_value_page_pv"></span></span> </div> 
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script> let s = document.createElement('script'); s.src = '//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js'; s.onload = () => document.getElementById('busuanzi_stats').style.display = 'inline-block'; document.head.appendChild(s); </script>


<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
